##Part B##
##Submitted by: Tim Ribaric##

###Additional packages###
Most of the code is vanilla Python however the following extra packages are required:

	- deap - Distributed Evolutional Algorithms in Python (https://github.com/DEAP/deap)
	- numpy - Python Package to handle heavy numeric calculations (http://www.numpy.org/)
	- matplotlib - Python graphing package (http://matplotlib.org/)

There are some other helper scripts written in bash, they mostly manipulate the data for it can be analyzed for the report

###Producing the Data###

1. As with part A, `assign1_part_B.py` is where most of the activity is done.  here gp are created and testing against training data and logged, as well as testing of best individual found per run is generated and logged.
2. The `logbook` objects generated by DEAP are then saved in the _logs_ directory. Details about training and testing are saved in the _data_ directory. Similarly the best GP individuals found in each run and details about them are written to `best_sols.txt`
3. Run `gen_data_part_b` to reset all log files, execute 20 runs of GP and write out some text files that allow for an easy calculation of testing scores and for creating confusion matrices for the category test data.
4. You can pipe the output of `generate_analysis_data` in a text file to get a snap shot of all the relevant data for the run

###Other files###

- `best_red` - data gathered for best run on Red Wine data
- `best_white` - data gathered for best run on White wine data
- `blast_out` - bash script that resets logs and other files
- `generate_analsyis_data` - bash script that outputs data in one file for confusion matrix testing. Also runs a few scripts explained subsequently
- `graph_fitness.py` - similar to Part A
- `paramSet` - outlines the parameters of the GP run
- `review_confusion_categories_good_bad.py` - creates confusion matrix for tesing wine as good/bad
- `review_test_good_bad.py` - calculates per run the % accuracy of the good/bad wine scoring
- `review_test_match_value.py` - calculates per run the % accuracy of the actual score v calculated score
- `smash_columns` - similar to Part A
- `trunk_logs` - similar to Part A
- `winequality-red.csv` - retrieved from: http://archive.ics.uci.edu/ml/datasets/Wine+Quality
- `winequality-white.csv` - retrieved from:
http://archive.ics.uci.edu/ml/datasets/Wine+Quality
